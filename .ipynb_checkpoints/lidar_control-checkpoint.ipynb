{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Ready\n"
     ]
    }
   ],
   "source": [
    "from lidar_control import lidar_control\n",
    "import pyrealsense2 as rs                 # Intel RealSense cross-platform open-source API\n",
    "import json\n",
    "\n",
    "print(\"Environment Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started Intel RealSense L515 S/N: 00000000f0220140\n",
      "[<pyrealsense2.pyrealsense2.sensor object at 0x0000017BB11C9228>, <pyrealsense2.pyrealsense2.sensor object at 0x0000017BB11C9298>, <pyrealsense2.pyrealsense2.sensor object at 0x0000017BB11C9570>]\n"
     ]
    }
   ],
   "source": [
    "l = lidar_control()\n",
    "l.load_settings('short_range_settings.json')\n",
    "l.init(stop=False)\n",
    "#camera = l.read_from_camera()\n",
    "self = l\n",
    "device = l.active_devices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conf = self.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pipe': <pyrealsense2.pyrealsense2.pipeline object at 0x0000017BB11C93E8>, 'device_info': {'name': 'Intel RealSense L515', 'serial_number': '00000000f0220140', 'firmware_version': '01.04.01.00', 'recommended_firmware_version': '01.04.01.02', 'physical_port': '\\\\\\\\?\\\\usb#vid_8086&pid_0b64&mi_04#6&13b51aa1&0&0004#{e5323777-f976-4f5b-9b55-b94699c46e44}\\\\global', 'debug_op_code': '15', 'product_id': '0B64', 'camera_locked': 'YES', 'usb_type_descriptor': '3.2', 'product_line': 'L500', 'asic_serial_number': '00000003a9d3c14d', 'firmware_update_id': '00000003a9d3c14d'}, 'pipeline_profile': <pyrealsense2.pyrealsense2.pipeline_profile object at 0x0000017BB11C9538>}\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "\n",
    "x='''print(i.query_sensors())\n",
    "print(l.sensors)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.context.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in rs.camera_info.__members__:\n",
    "    rs.camera_info.__getattr__[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "self = l\n",
    "def get_sensor_info(dev):\n",
    "        camera_info = {}\n",
    "        for k in rs.camera_info.__members__.keys():\n",
    "            try:\n",
    "                camera_info[k] = dev.get_info(rs.camera_info.__members__[k])\n",
    "            except:\n",
    "                pass\n",
    "        return(camera_info)\n",
    "l.get_sensor_info = get_sensor_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.dev = self.context.query_devices()\n",
    "pipelines = []\n",
    "dev_infos = []\n",
    "pipeline_profiles = []\n",
    "for d in self.dev:\n",
    "            pipe = rs.pipeline(self.context)\n",
    "            dev_info = self.get_sensor_info(d)\n",
    "            self.config.enable_device(dev_info['serial_number'])\n",
    "            pipeline_profile = self.start(pipeline=pipe,config=self.config)\n",
    "            if (self.verbose)\n",
    "                print(f'started {dev_info[\"name\"]} S/N: {dev_info[\"serial_number\"]}')\n",
    "            # store in list\n",
    "            pipelines.append(pipe)\n",
    "            dev_infos.append(dev_info)\n",
    "            pipeline_profiles.append(pipeline_profile)\n",
    "            \n",
    "for d in self.dev:\n",
    "            \n",
    "            self.stop(pipeline=pipe)\n",
    "            print(f'stopped {dev_info[\"name\"]} S/N: {dev_info[\"serial_number\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "camera = l.read_from_camera()\n",
    "\n",
    "camera['pc'] = l.get_pointcloud(camera['depth'],\\\n",
    "                      camera['color'],\\\n",
    "                      pc_file='points.ply')\n",
    "\n",
    "frames = [camera['depth'],camera['nir'],camera['color'],camera['colorized_depth']]\n",
    "l.plot(frames,dummy=True,file='result.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.camera_info.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(rs_dev.get_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs2::config config;             \n",
    "         rs2::pipeline_profile pipeline_profile = pipeline.start(config); // camera starts capturing\n",
    "         pipeline_profile = pipeline.start(config);\n",
    "         rs2::device rs_dev = pipeline_profile.get_device();\n",
    "         std::cout <<\"Device Name\"<<\": \"<< rs_dev.get_info(RS2_CAMERA_INFO_NAME)<<std::endl;\n",
    "         std::cout <<\"Firmware Version\"<<\": \"<<rs_dev.get_info(RS2_CAMERA_INFO_FIRMWARE_VERSION)<<std::endl;\n",
    "         std::cout <<\"Serial Number\"<<\": \"<<rs_dev.get_info(RS2_CAMERA_INFO_SERIAL_NUMBER)<<std::endl;\n",
    "         std::cout <<\"Product Id\"<<\": \"<<rs_dev.get_info(RS2_CAMERA_INFO_PRODUCT_ID)<<std::endl;\n",
    "         pipeline.stop(); // camera stops capturing     \n",
    "         ----   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = rs.context.query_devices\n",
    "p = rs.pipeline(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conf = rs.config()\n",
    "help(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyglet_pointcloud_viewer.py\n",
    "# Processing blocks\n",
    "def convert_fmt(fmt):\n",
    "    \"\"\"rs.format to pyglet format string\"\"\"\n",
    "    return {\n",
    "        rs.format.rgb8: 'RGB',\n",
    "        rs.format.bgr8: 'BGR',\n",
    "        rs.format.rgba8: 'RGBA',\n",
    "        rs.format.bgra8: 'BGRA',\n",
    "        rs.format.y8: 'L',\n",
    "    }[fmt]\n",
    "\n",
    "\n",
    "\n",
    "def read_from_camera(decimate_scale=0,postprocessing=False,color=False):\n",
    "    #start the frames pipe\n",
    "    p = rs.pipeline()\n",
    "    conf = rs.config()\n",
    "    conf.enable_stream(rs.stream.depth, 1024, 768, rs.format.z16, 30)\n",
    "    other_nir_stream, other_nir_format = rs.stream.infrared, rs.format.y8\n",
    "    other_stream, other_format = rs.stream.color, rs.format.rgb8\n",
    "    conf.enable_stream(other_stream, 1920, 1080, other_format, 30)\n",
    "    conf.enable_stream(other_nir_stream, 1024, 768, other_nir_format, 30)\n",
    "    conf.enable_stream(rs.stream.accel,rs.format.motion_xyz32f,200)\n",
    "    conf.enable_stream(rs.stream.gyro,rs.format.motion_xyz32f,200)\n",
    "    \n",
    "    try:\n",
    "        prof = p.start(conf)\n",
    "    except:\n",
    "        print('failed to start connection to device')\n",
    "        return None,None,None\n",
    "    \n",
    "    profile = p.get_active_profile()\n",
    "    depth_sensor = profile.get_device().first_depth_sensor()\n",
    "    depth_scale = depth_sensor.get_depth_scale()\n",
    "    depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))\n",
    "    depth_intrinsics = depth_profile.get_intrinsics()\n",
    "    w, h = depth_intrinsics.width, depth_intrinsics.height\n",
    "    # from pyglet_pointcloud_viewer.py\n",
    "    # Processing blocks\n",
    "    pc = rs.pointcloud()\n",
    "    if decimate_scale > 0:\n",
    "        decimate = rs.decimation_filter()\n",
    "        decimate.set_option(rs.option.filter_magnitude, int(2 ** decimate_level))\n",
    "        \n",
    "    colorizer = rs.colorizer()\n",
    "    \n",
    "    vertex_list = pyglet.graphics.vertex_list(\n",
    "    w * h, 'v3f/stream', 't2f/stream', 'n3f/stream')\n",
    "    \n",
    "    if postprocessing:\n",
    "        filters = [rs.disparity_transform(),\n",
    "               rs.spatial_filter(),\n",
    "               rs.temporal_filter(),\n",
    "               rs.disparity_transform(False)]\n",
    "\n",
    "    frames = p.wait_for_frames()\n",
    "    print(frames is not None)\n",
    "    print(frames)\n",
    "    if not frames:\n",
    "        return None,None,None,None\n",
    "\n",
    "    depth_frame = frames.get_depth_frame().as_video_frame()\n",
    "    other_frame = frames.first(other_stream).as_video_frame()\n",
    "    other_nir_frame = frames.first(other_nir_stream).as_video_frame()\n",
    "\n",
    "    if decimate_scale > 0:\n",
    "        depth_frame = decimate.process(depth_frame)\n",
    "\n",
    "    if postprocessing:\n",
    "        for f in filters:\n",
    "            depth_frame = f.process(depth_frame)\n",
    "\n",
    "    # Grab new intrinsics (may be changed by decimation)\n",
    "    depth_intrinsics = rs.video_stream_profile(depth_frame.profile).get_intrinsics()\n",
    "    w, h = depth_intrinsics.width, depth_intrinsics.height\n",
    "\n",
    "    color_data = np.asanyarray(other_frame.get_data())\n",
    "    depth_data = np.asanyarray(depth_frame.get_data())\n",
    "    nir_data = np.asanyarray(other_nir_frame.get_data())\n",
    "\n",
    "    colorized_depth = colorizer.colorize(depth_frame)\n",
    "    depth_colormap = np.asanyarray(colorized_depth.get_data())\n",
    "\n",
    "    if color:\n",
    "        mapped_frame, color_source = other_frame, color_image\n",
    "    else:\n",
    "        mapped_frame, color_source = colorized_depth, depth_colormap\n",
    "\n",
    "        \n",
    "    points = pc.calculate(depth_frame)\n",
    "    pc.map_to(mapped_frame)\n",
    "\n",
    "    # handle color source or size change\n",
    "    fmt = convert_fmt(mapped_frame.profile.format())\n",
    "    other_profile = rs.video_stream_profile(profile.get_stream(other_stream))\n",
    "    \n",
    "    global image_data\n",
    "    if (image_data.format, image_data.pitch) != (fmt, color_source.strides[0]):\n",
    "        empty = (gl.GLubyte * (w * h * 3))()\n",
    "        image_data = pyglet.image.ImageData(w, h, fmt, empty)\n",
    "    # copy image data to pyglet\n",
    "    image_data.set_data(fmt, color_source.strides[0], color_source.ctypes.data)\n",
    "\n",
    "    verts = np.asarray(points.get_vertices(2)).reshape(h, w, 3)\n",
    "    texcoords = np.asarray(points.get_texture_coordinates(2))\n",
    "\n",
    "    if len(vertex_list.vertices) != verts.size:\n",
    "        vertex_list.resize(verts.size // 3)\n",
    "        # need to reassign after resizing\n",
    "        vertex_list.vertices = verts.ravel()\n",
    "        vertex_list.tex_coords = texcoords.ravel()\n",
    "\n",
    "    # copy our data to pre-allocated buffers, this is faster than assigning...\n",
    "    # pyglet will take care of uploading to GPU\n",
    "    def copy(dst, src):\n",
    "        \"\"\"copy numpy array to pyglet array\"\"\"\n",
    "        # timeit was mostly inconclusive, favoring slice assignment for safety\n",
    "        np.array(dst, copy=False)[:] = src.ravel()\n",
    "        # ctypes.memmove(dst, src.ctypes.data, src.nbytes)\n",
    "\n",
    "    copy(vertex_list.vertices, verts)\n",
    "    copy(vertex_list.tex_coords, texcoords)\n",
    "\n",
    "    if state.lighting:\n",
    "        # compute normals\n",
    "        dy, dx = np.gradient(verts, axis=(0, 1))\n",
    "        n = np.cross(dx, dy)\n",
    "\n",
    "        # can use this, np.linalg.norm or similar to normalize, but OpenGL can do this for us, see GL_NORMALIZE above\n",
    "        # norm = np.sqrt((n*n).sum(axis=2, keepdims=True))\n",
    "        # np.divide(n, norm, out=n, where=norm != 0)\n",
    "\n",
    "        # import cv2\n",
    "        # n = cv2.bilateralFilter(n, 5, 1, 1)\n",
    "\n",
    "        copy(vertex_list.normals, n)\n",
    "\n",
    "    if keys[pyglet.window.key.E]:\n",
    "        points.export_to_ply('./out.ply', mapped_frame)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return p,[depth_data,nir_data,color_data,color_source],points,(depth_sensor,depth_scale,depth_profile,depth_intrinsics)\n",
    "\n",
    "p,f,points,d = read_from_camera()\n",
    "\n",
    "if p:\n",
    "    print('Measured')\n",
    "    (depth_sensor,depth_scale,depth_profile,depth_intrinsics) = d\n",
    "\n",
    "    result = [i for i in [np.asanyarray(frame) for frame in f] if len(i)]\n",
    "    '''result[0] = depth_scale * result[0]'''\n",
    "    p.stop()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "nr = (2 * ((len (result) +1)// 2))\n",
    "print(nr)\n",
    "nx = nr-nr//2\n",
    "shape = (nx,nr//nx)\n",
    "fig, axs = plt.subplots(shape[1],shape[0],figsize=(20,20))\n",
    "axs = np.array(axs).flatten()\n",
    "\n",
    "cmaps = [None,plt.get_cmap('gray'),None,None]\n",
    "titles = ['range','NIR','colour','colourised depth']\n",
    "print (shape)\n",
    "centiles = [(10,70),(25,95),(25,75),(5,95)]\n",
    "for i in range(nr):\n",
    "    try:\n",
    "        r = np.array(result[i].copy())\n",
    "        '''r[r==0] = np.nan'''\n",
    "        r_reste = r[r>0]\n",
    "        rmin = np.percentile(r_reste,centiles[i][0])\n",
    "        rmax = np.percentile(r,centiles[i][1])\n",
    "\n",
    "        im = axs[i].imshow(r,vmin=rmin,vmax=rmax,cmap=cmaps[i])\n",
    "        axs[i].title.set_text(titles[i])\n",
    "        if i != 2:\n",
    "            divider = make_axes_locatable(axs[i])\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "            fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "fig.savefig(\"first_light.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = depth_intrinsics.width, depth_intrinsics.height\n",
    "print(w,h)\n",
    "\n",
    "# Processing blocks\n",
    "pc = rs.pointcloud()\n",
    "colorizer = rs.colorizer()\n",
    "filters = [rs.disparity_transform(),\n",
    "           rs.spatial_filter(),\n",
    "           rs.temporal_filter(),\n",
    "           rs.disparity_transform(False)]\n",
    "\n",
    "other_profile = rs.video_stream_profile(profile.get_stream(other_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Skip 5 first frames to give the Auto-Exposure time to adjust\n",
    "for x in range(5):\n",
    "  pipeline.wait_for_frames()\n",
    "  \n",
    "# Store next frameset for later processing:\n",
    "frameset = pipeline.wait_for_frames()\n",
    "color_frame = frameset.get_color_frame()\n",
    "depth_frame = frameset.get_depth_frame()\n",
    "\n",
    "# Cleanup:\n",
    "pipeline.stop()\n",
    "print(\"Frames Captured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup:\n",
    "pipe = rs.pipeline()\n",
    "cfg = rs.config()\n",
    "cfg.enable_device_from_file(\"object_detection.bag\")\n",
    "profile = pipe.start(cfg)\n",
    "\n",
    "# Skip 5 first frames to give the Auto-Exposure time to adjust\n",
    "for x in range(5):\n",
    "  pipe.wait_for_frames()\n",
    "  \n",
    "# Store next frameset for later processing:\n",
    "frameset = pipe.wait_for_frames()\n",
    "color_frame = frameset.get_color_frame()\n",
    "depth_frame = frameset.get_depth_frame()\n",
    "\n",
    "# Cleanup:\n",
    "pipe.stop()\n",
    "print(\"Frames Captured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = np.asanyarray(color_frame.get_data())\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.imshow(color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorizer = rs.colorizer()\n",
    "colorized_depth = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "plt.imshow(colorized_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create alignment primitive with color as its target stream:\n",
    "align = rs.align(rs.stream.color)\n",
    "frameset = align.process(frameset)\n",
    "\n",
    "# Update color and depth frames:\n",
    "aligned_depth_frame = frameset.get_depth_frame()\n",
    "colorized_depth = np.asanyarray(colorizer.colorize(aligned_depth_frame).get_data())\n",
    "\n",
    "# Show the two frames together:\n",
    "images = np.hstack((color, colorized_depth))\n",
    "plt.imshow(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard OpenCV boilerplate for running the net:\n",
    "height, width = color.shape[:2]\n",
    "expected = 300\n",
    "aspect = width / height\n",
    "resized_image = cv2.resize(color, (round(expected * aspect), expected))\n",
    "crop_start = round(expected * (aspect - 1) / 2)\n",
    "crop_img = resized_image[0:expected, crop_start:crop_start+expected]\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(\"MobileNetSSD_deploy.prototxt\", \"MobileNetSSD_deploy.caffemodel\")\n",
    "inScaleFactor = 0.007843\n",
    "meanVal       = 127.53\n",
    "classNames = (\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "              \"bottle\", \"bus\", \"car\", \"cat\", \"chair\",\n",
    "              \"cow\", \"diningtable\", \"dog\", \"horse\",\n",
    "              \"motorbike\", \"person\", \"pottedplant\",\n",
    "              \"sheep\", \"sofa\", \"train\", \"tvmonitor\")\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(crop_img, inScaleFactor, (expected, expected), meanVal, False)\n",
    "net.setInput(blob, \"data\")\n",
    "detections = net.forward(\"detection_out\")\n",
    "\n",
    "label = detections[0,0,0,1]\n",
    "conf  = detections[0,0,0,2]\n",
    "xmin  = detections[0,0,0,3]\n",
    "ymin  = detections[0,0,0,4]\n",
    "xmax  = detections[0,0,0,5]\n",
    "ymax  = detections[0,0,0,6]\n",
    "\n",
    "className = classNames[int(label)]\n",
    "\n",
    "cv2.rectangle(crop_img, (int(xmin * expected), int(ymin * expected)), \n",
    "             (int(xmax * expected), int(ymax * expected)), (255, 255, 255), 2)\n",
    "cv2.putText(crop_img, className, \n",
    "            (int(xmin * expected), int(ymin * expected) - 5),\n",
    "            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255,255,255))\n",
    "\n",
    "plt.imshow(crop_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = height / expected\n",
    "xmin_depth = int((xmin * expected + crop_start) * scale)\n",
    "ymin_depth = int((ymin * expected) * scale)\n",
    "xmax_depth = int((xmax * expected + crop_start) * scale)\n",
    "ymax_depth = int((ymax * expected) * scale)\n",
    "xmin_depth,ymin_depth,xmax_depth,ymax_depth\n",
    "cv2.rectangle(colorized_depth, (xmin_depth, ymin_depth), \n",
    "             (xmax_depth, ymax_depth), (255, 255, 255), 2)\n",
    "plt.imshow(colorized_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = np.asanyarray(aligned_depth_frame.get_data())\n",
    "# Crop depth data:\n",
    "depth = depth[xmin_depth:xmax_depth,ymin_depth:ymax_depth].astype(float)\n",
    "\n",
    "# Get data scale from the device and convert to meters\n",
    "depth_scale = profile.get_device().first_depth_sensor().get_depth_scale()\n",
    "depth = depth * depth_scale\n",
    "dist,_,_,_ = cv2.mean(depth)\n",
    "print(\"Detected a {0} {1:.3} meters away.\".format(className, dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
